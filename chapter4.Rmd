# Rstudio Exercise 4

### Description of the data used for the exercise

This week we use the **Boston** dataset which is included in the MASS package. The dataset is collected by the U.S Census Servise and it contains information about housing in the area of Boston Mass. There are 506 cases and 14 variables in the dataset. 

```{r, message=FALSE,warning=FALSE}
# accessing the MASS package
library(MASS)

# loading the data
data("Boston")

# exploring the structure and dimensions
str(Boston)
dim(Boston)
```

### Graphical overview of the data and relationships between variables

Below are shown the summaries and histograms of each variable. The variables are not very normaly distributed. One of the variables (chas) is categorical. The value 0 means that tract does not bound Charles River. Only very few observations have a value 1 in chas-variable.

```{r, echo=FALSE, message=FALSE,warning=FALSE}
library(corrplot); library(dplyr);library(tidyr); library(ggplot2)

summary(Boston)

# drawing a histogram of each variable
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_histogram()
```

Below the relationships of the variables are shown in a correlation matrix. Strong negative correaltions are found between dis (weighted distances to five Boston employment centres) and age (proportion of owner-occupied units built prior to 1940), nox (nitric oxided concentration, and indus (proportion of non-retail business acres per town). This means that if the distance from the employment centres increases, the housing units are younger, there is less nitric oxid and less non-retail business acres. A quite strong negative correlation was also found between medv (median value of owner-occupied houses) and lstat (% lower status of the population), meaning that value of the houses increases there is less lower status population. Highest positive correaltion was found between rad and tax. Meaning that when accessibility to radial highways inreases, also the property tax-rate increases.

```{r, echo=FALSE, message=FALSE,warning=FALSE}

# calculating and the correlation matrix
cor_matrix<-cor(Boston) %>% round(2)
cor_matrix 

# visualizing the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

### Scaling the Boston dataset

Scaling the Boston dataset changed original variables so, that the mean is 0. The histograms show that scaling did not alter the distributions.

```{r, echo=FALSE, message=FALSE,warning=FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)
summary(boston_scaled)

# saving the scaled dataset as a data frame
boston_scaled <- as.data.frame(boston_scaled)

# drawing a histogram of each variable
gather(boston_scaled) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_histogram()
```

### Creating the categorical crime variable and dividing the dataset

Frequencies of the new categorical crime-variable:

```{r}

# creating a quantile vector of crim 
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# removing original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# adding the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

table(boston_scaled$crime)
```

Next, the original data is divided into two datasets: "train" and "test". The "train" dataset includes 80 % of the original observations and we used it to create a model to predict crime class. Then we test the model to predict the crime class in the "test" dataset. 

```{r}
#Dividing the dataset to two: test & train
# number of rows in the Boston dataset 
n <- nrow(Boston)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
```

### Linear discriminant analysis

```{r, echo=FALSE, message=FALSE,warning=FALSE}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

### Predicting the classes with the LDA model on the test data.

Cross-tabulation of the correct vs. predicted crime classes in the test data:

```{r, warning=FALSE, error=FALSE}

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

From the cross-tabluation we can see that the model is quite good at predicting the high class, only one observation in med_high class is incorrectly predicted to be in high class. However, also in the other classes more than half of the observations are correctly classified using the prediction. 

### K-means clustering

##### **Distances**

```{r, echo=FALSE}
library(MASS)
data('Boston')
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)

# euclidean distance matrix for scaled variables
dist_eu <- dist(boston_scaled)
summary(dist_eu)
```

##### **Determining the number of clusters**

```{r, echo=FALSE}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

The optimal number of clusters seems to be 2, because the line flattens after that.

##### **Plotting clusters**

Below the plotted clusters are presented. To be honest, I can't interpret the plots. I hope that I learn during peer review exercise :)

```{r, echo=FALSE}
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```

